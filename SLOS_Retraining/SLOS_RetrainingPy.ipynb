{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc70034-96f3-4c28-af89-14c817ef6cc5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook provides an interactive environment for training a predictive model to estimate patient length of stay (LOS). This workflow is particularly suited for healthcare professionals and researchers interested in ICU resource planning and patient flow management.\n",
    "\n",
    "Users are only requested to change the code block indicated with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4594dc8-9731-4a54-aeb7-8eea93d6e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ‼️User Action Required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc959be2-8e9c-4d08-873a-71c48474a97e",
   "metadata": {},
   "source": [
    "### Package Installation\n",
    "This code checks for the presence of required R packages and installs them if they are not already available.\n",
    "These packages are essential for data preprocessing, model training, ensemble learning, and performance evaluation:\n",
    "- `caret`: Core package for building and tuning predictive models.\n",
    "- `caretEnsemble`: Allows combining multiple caret models into an ensemble for improved accuracy.\n",
    "- `tidyverse`: Collection of packages for data manipulation, visualization, and general workflow.\n",
    "- `MLmetrics`: Provides machine learning evaluation metrics (e.g., MAE, RMSE).\n",
    "- `ranger`: Fast implementation of Random Forests, useful for training tree-based models efficiently.\n",
    "\n",
    "⚠️ You only need to run this block once per session or when setting up a new environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b809a57a-eac6-4806-845d-cc223ff31648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadr \n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, cross_val_predict \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import chi2_contingency, variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e9e9f-24ad-4f3a-b30e-25572191763d",
   "metadata": {},
   "source": [
    "### Load and Validate User Dataset\n",
    "\n",
    "1. Please change the `data_path` the **path to the data** you want to train the model on. It can be in `.csv` or `.RData` format. If it's an `R.Data` file, please include the object name in the `object_name` variable.\n",
    "\n",
    "2. If you want to include your **own predictors**, please change the `predictors` variable to include your a dataframe with one column stating the names of your predictors. If not, the the list of predictor variables used during the original model training is automatically loaded.\n",
    "\n",
    "3. Add the **name** you want the model to be saved as to the `user_model_name` variable. the model will be saved as an .pkl file\n",
    "\n",
    "⚠️ If not using your own predictors, make sure your dataset includes all required predictors listed in predictors.csv, as well as the target variable UnitLengthStay_trunc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "012859e2-7130-4c08-8090-3c388deefab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ‼️User Action Required\n",
    "\n",
    "data_path = \"C:\\\\Users\\\\joana\\\\Documentos\\\\SLOS\\\\SLOS retraining\\\\SampledData.csv\"\n",
    "object_name = \"sampled_data\"\n",
    "user_model_name = \"YOUR_MODEL_NAME\"\n",
    "predictors = \"C:\\\\Users\\\\joana\\\\Documentos\\\\SLOS\\\\SLOS retraining\\\\predictors.csv\" # default predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe9e716-2ea2-4260-91f1-946fda030880",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_path.lower().endswith(\".csv\"):\n",
    "    user_data = pd.read_csv(data_path)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported file type. Please upload a .csv file.\")\n",
    "\n",
    "predictors_df = pd.read_csv(predictors)\n",
    "predictors = predictors_df.iloc[:, 1].tolist()\n",
    "\n",
    "if not all(p in user_data.columns for p in predictors):\n",
    "    raise ValueError(\"Some required predictors are missing in your dataset.\")\n",
    "\n",
    "if 'UnitLengthStay_trunc' not in user_data.columns:\n",
    "    raise ValueError(\"Target variable 'UnitLengthStay_trunc' is missing in the dataset.\")\n",
    "\n",
    "columns_to_select = predictors + ['UnitLengthStay_trunc'] + ['UnitLengthStay']\n",
    "user_data = user_data[columns_to_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d63e5-07dd-4a9d-b3eb-817e1b9f85d0",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "We remove zero and near-zero variance features, correlated predictors (for numeric and categorical features) and we impute missing data via the MICE algortihm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bfae825-b7be-4efc-ac25-dd0c2701c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(998)\n",
    "train_indices = train_test_split(\n",
    "    range(len(user_data)), \n",
    "    train_size=0.8, \n",
    "    random_state=998,\n",
    "    stratify=None  \n",
    ")[0]\n",
    "inTraining = train_indices\n",
    "\n",
    "training = user_data.iloc[inTraining].copy()\n",
    "training_dummy = training.copy()\n",
    "testing = user_data.drop(user_data.index[inTraining]).copy()\n",
    "testing_dummy = testing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26baa930-e4dd-48b8-8dfd-86bdebec8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highly_correlated(df, threshold=0.75):\n",
    "    corr_matrix = df.corr(method='pearson').abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return to_drop\n",
    "\n",
    "numeric_cols = training.select_dtypes(include=[np.number]).columns.difference([\"UnitLengthStay_trunc\"])\n",
    "high_corr_numeric = find_highly_correlated(training[numeric_cols])\n",
    "\n",
    "train_data = training.drop(columns=high_corr_numeric)\n",
    "test_data = testing.drop(columns=high_corr_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5fd45fd-1c1f-4512-acd6-04c505a4b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    if confusion_matrix.shape[0] < 2 or confusion_matrix.shape[1] < 2:\n",
    "        return 0\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1)*(r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1)**2) / (n - 1)\n",
    "    kcorr = k - ((k - 1)**2) / (n - 1)\n",
    "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "\n",
    "cat_cols = train_data.select_dtypes(include='object').columns\n",
    "cramer_matrix = pd.DataFrame(index=cat_cols, columns=cat_cols)\n",
    "\n",
    "for col1, col2 in itertools.combinations(cat_cols, 2):\n",
    "    val = cramers_v(train_data[col1], train_data[col2])\n",
    "    cramer_matrix.loc[col1, col2] = val\n",
    "    cramer_matrix.loc[col2, col1] = val\n",
    "np.fill_diagonal(cramer_matrix.values.astype(float), 0)\n",
    "\n",
    "high_corr_cat = []\n",
    "for col in cramer_matrix.columns:\n",
    "    if any(cramer_matrix[col].astype(float) > 0.5):\n",
    "        high_corr_cat.append(col)\n",
    "\n",
    "train_data = train_data.drop(columns=high_corr_cat)\n",
    "test_data = test_data.drop(columns=high_corr_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feca4532-7bff-45e3-ae92-c39445be5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mice_imputation(df, target_col='UnitLengthStay_trunc', random_state=100, max_iter=5):\n",
    "    df_imp = df.copy()\n",
    "\n",
    "    numeric_cols = df_imp.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    categorical_cols = df_imp.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if numeric_cols:\n",
    "        numeric_imputer = IterativeImputer(\n",
    "            estimator=RandomForestRegressor(random_state=random_state, n_estimators=10),\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        df_imp[numeric_cols] = numeric_imputer.fit_transform(df_imp[numeric_cols])\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        mode_value = df_imp[col].mode()\n",
    "        if len(mode_value) > 0:\n",
    "            df_imp[col] = df_imp[col].fillna(mode_value[0])\n",
    "    \n",
    "    return df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d64703b4-688e-4407-bdd6-44c1a122909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "training_imp = mice_imputation(training.drop(columns=['UnitLengthStay_trunc']), random_state=100)\n",
    "training_imp['UnitLengthStay_trunc'] = training_dummy['UnitLengthStay_trunc']\n",
    "training = training_imp.copy()\n",
    "\n",
    "np.random.seed(100)\n",
    "testing_imp = mice_imputation(testing.drop(columns=['UnitLengthStay_trunc']), random_state=100)\n",
    "testing_imp['UnitLengthStay_trunc'] = testing_dummy['UnitLengthStay_trunc']\n",
    "testing = testing_imp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b370b3a-4608-4104-be6c-b6388a2a6ff0",
   "metadata": {},
   "source": [
    "### Model Traning\n",
    "This section covers the full training pipeline, from splitting the data to building an ensemble model using sklearn.ensemble.\n",
    " \n",
    "Two base models are trained: Linear regression (sklearn.linear_model) and Random Forest (sklearn.ensemble).\n",
    "\n",
    "A stacked ensemble model is built from the base learners.The final stacked model is saved as \"stacked_best_model\", and all of the models are saved in the .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4db4d18-9548-4952-89e5-a1032aa01837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train = training.drop(columns=['UnitLengthStay_trunc'])\n",
    "y_train = training['UnitLengthStay_trunc']\n",
    "X_test = testing.drop(columns=['UnitLengthStay_trunc'])\n",
    "y_test = testing['UnitLengthStay_trunc']\n",
    "\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_cols),  # Keep numerical columns as-is\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names = []\n",
    "feature_names.extend(numerical_cols)\n",
    "if categorical_cols:\n",
    "    cat_encoder = preprocessor.named_transformers_['cat']\n",
    "    cat_feature_names = cat_encoder.get_feature_names_out(categorical_cols)\n",
    "    feature_names.extend(cat_feature_names)\n",
    "\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "\n",
    "cv_folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), \n",
    "                         greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a6ab15-a67e-4d2d-99df-4941c2d4aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model = LinearRegression()\n",
    "lm_cv_scores = cross_val_score(lm_model, X_train_processed, y_train, \n",
    "                              cv=cv_folds, scoring=rmse_scorer, verbose=1)\n",
    "lm_model.fit(X_train_processed, y_train)\n",
    "lm_rmse = -lm_cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20ff992f-182d-41a9-9765-928c18b51fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [100],  # Fixed for initial training\n",
    "    'max_features': list(range(5, min(11, len(feature_names)+1))),  # mtry equivalent, adjusted for actual feature count\n",
    "    'min_samples_split': [10],  # Approximation of min.node.size\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_model, \n",
    "    rf_param_grid, \n",
    "    cv=cv_folds, \n",
    "    scoring=rmse_scorer,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_grid_search.fit(X_train_processed, y_train)\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "rf_rmse = -rf_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeee3420-ed62-431c-bf17-06d9225ce273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Models saved to  YOUR_MODEL_NAME.pkl\n"
     ]
    }
   ],
   "source": [
    "model_list = {\n",
    "    'lm': lm_model,\n",
    "    'rf': rf_best_model\n",
    "}\n",
    "\n",
    "lm_pred_train = cross_val_predict(lm_model, X_train_processed, y_train, cv=cv_folds)\n",
    "rf_pred_train = cross_val_predict(rf_best_model, X_train_processed, y_train, cv=cv_folds)\n",
    "\n",
    "stacking_features = pd.DataFrame({\n",
    "    'lm_pred': lm_pred_train,\n",
    "    'rf_pred': rf_pred_train\n",
    "})\n",
    "\n",
    "stacked_param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_features': [2],  # mtry = 2\n",
    "    'min_samples_split': [10, 20, 30, 40],  # min.node.size equivalent\n",
    "    'criterion': ['squared_error'],  # variance equivalent\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "stacked_model = RandomForestRegressor()\n",
    "stacked_grid_search = GridSearchCV(\n",
    "    stacked_model,\n",
    "    stacked_param_grid,\n",
    "    cv=cv_folds,\n",
    "    scoring=rmse_scorer,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacked_grid_search.fit(stacking_features, y_train)\n",
    "stacked_best_model = stacked_grid_search.best_estimator_\n",
    "stacked_rmse = -stacked_grid_search.best_score_\n",
    "\n",
    "models = {\n",
    "    'individual_models': model_list,\n",
    "    'stacked_model': stacked_best_model,\n",
    "    'stacking_features_columns': stacking_features.columns.tolist()\n",
    "}\n",
    "\n",
    "if user_model_name[-4] != \".pkl\":\n",
    "    user_model_name += \".pkl\"\n",
    "joblib.dump(models, user_model_name)\n",
    "print(\"Models saved to \", user_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51665ef5-3412-4489-976a-dd0d613992a6",
   "metadata": {},
   "source": [
    "### Model Prediction and Evaluation\n",
    "This section handles making predictions with the trained stacked model and evaluating its performance using key metrics.\n",
    "\n",
    "1. the `predict_stacked` call generates **predictions** for the test set using the stacked model.\n",
    "\n",
    "2. We **evaluate** the trained model performance on three metrics:\n",
    "\n",
    "- Root Mean Squared Error (RMSE): Measures the average magnitude of the prediction errors.\n",
    "\n",
    "- Mean Absolute Error (MAE): Measures the average of the absolute errors, giving a sense of how far off the predictions are.\n",
    "\n",
    "- R-squared (R2): Indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "These metrics are computed using the functions available in the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a371a9e2-bec2-47b4-ad08-8b210e649917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stacked(X, models, preprocessor):\n",
    "        \n",
    "    X_processed = preprocessor.transform(X)\n",
    "    \n",
    "    lm_pred = models['individual_models']['lm'].predict(X_processed)\n",
    "    rf_pred = models['individual_models']['rf'].predict(X_processed)\n",
    "    \n",
    "    stacking_input = pd.DataFrame({\n",
    "        'lm_pred': lm_pred,\n",
    "        'rf_pred': rf_pred\n",
    "    })\n",
    "    \n",
    "    final_pred = models['stacked_model'].predict(stacking_input)\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cfe2dc4-f890-416d-8f76-b399fd553cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5264\n",
      "MAE: 0.3819\n",
      "R²: 0.9880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "C:\\Users\\joana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\joana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_predictions = predict_stacked(X_test, models, preprocessor)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"RMSE: {test_rmse:.4f}\")\n",
    "print(f\"MAE: {test_mae:.4f}\")\n",
    "print(f\"R²: {test_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
